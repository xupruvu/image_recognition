{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b3e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as dset\n",
    "\n",
    "convnext_tiny = models.convnext_tiny(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "489cd167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU State: cuda:0\n",
      "訓練資料筆數為64323\n",
      "驗證資料筆數為16071\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)\n",
    "\n",
    "TrainImagePath = \"./train\"\n",
    "DevImagePath = \"./test\"\n",
    "TrainImageNum = len(os.listdir(TrainImagePath))\n",
    "DevImageNum = len(os.listdir(DevImagePath))\n",
    "print(\"訓練資料筆數為{0}\".format(TrainImageNum))\n",
    "print(\"驗證資料筆數為{0}\".format(DevImageNum))\n",
    "\n",
    "normedWeights = [0.9492872602433018, 0.8414185138194392, 0.975234470234097, 0.8797795855412095, 0.9756573873672164, 0.9048187675697191, 0.9412269572356146, 0.8691071472995497, 0.9003408214543374,\n",
    " 0.9791402343458467, 0.901087145806901, 0.9723486827375177, 0.9812796974898624, 0.9292733288553872]\n",
    "normedWeights = torch.FloatTensor(normedWeights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4ebfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropDataset(Dataset):\n",
    "    \"\"\"Crop dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, label_dict, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.label_dict = label_dict\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.listdir(self.root_dir)[idx]\n",
    "        image_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        label = self.label_dict[img_name.split(\"_\")[0]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb7389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler \n",
    "\n",
    "featuretransform = transforms.Compose([transforms.Resize(255), \n",
    "                                       transforms.CenterCrop(224),  \n",
    "#                                        transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(), \n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
    "\n",
    "label_dict = {\"banana\":0, \"bareland\":1, \"carrot\":2, \"corn\":3, \"dragonfruit\":4, \"garlic\":5, \"guava\":6, \"peanut\":7, \n",
    "                 \"pineapple\":8, \"pumpkin\":9, \"rice\":10, \"sugarcane\":11, \"tomato\":12, \"soybean\":13}\n",
    "\n",
    "# sampler = WeightedRandomSampler(weight_list, TrainImageNum, replacement=True)\n",
    "\n",
    "mongoDataset = CropDataset(TrainImagePath, label_dict, featuretransform)\n",
    "valDataset = CropDataset(DevImagePath, label_dict, featuretransform)\n",
    "\n",
    "# imgLoader = torch.utils.data.DataLoader(\n",
    "#          mongoDataset,batch_size= 128, shuffle= False, num_workers= 10,sampler=sampler)\n",
    "\n",
    "imgLoader = torch.utils.data.DataLoader(\n",
    "         mongoDataset,batch_size= 256, shuffle= False, num_workers= 10)\n",
    "\n",
    "valLoader = torch.utils.data.DataLoader(\n",
    "         valDataset,batch_size= 256, shuffle= False, num_workers= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebd38bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU State: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#fine tune the last classifier layer from 1000 to 15 dim(num. of classifier).\n",
    "convnext_tiny.classifier[2] = nn.Linear(768,14)\n",
    "convnext_tiny = convnext_tiny.cuda() #use GPU\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811e7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=normedWeights)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(convnext_tiny.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "model = convnext_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0764ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/252], Loss: 0.8647\n",
      "Epoch [1/20], Step [200/252], Loss: 0.3597\n",
      "Training Accuracy: 70.54708269203861 %\n",
      "Validation Accuracy on the 16070 validation images: 89.26638043681164 %\n",
      "Save Checkpoint!!\n",
      "\n",
      "Epoch [2/20], Step [100/252], Loss: 0.2420\n",
      "Epoch [2/20], Step [200/252], Loss: 0.0941\n",
      "Training Accuracy: 93.00872160813395 %\n",
      "Validation Accuracy on the 16070 validation images: 93.91450438678365 %\n",
      "Save Checkpoint!!\n",
      "\n",
      "Epoch [3/20], Step [100/252], Loss: 0.1877\n",
      "Epoch [3/20], Step [200/252], Loss: 0.1474\n",
      "Training Accuracy: 95.15569858371033 %\n",
      "Validation Accuracy on the 16070 validation images: 91.7615580859934 %\n",
      "\n",
      "Epoch [4/20], Step [100/252], Loss: 0.1262\n",
      "Epoch [4/20], Step [200/252], Loss: 0.0941\n",
      "Training Accuracy: 96.29992382196104 %\n",
      "Validation Accuracy on the 16070 validation images: 95.0967581357725 %\n",
      "Save Checkpoint!!\n",
      "\n",
      "Epoch [5/20], Step [100/252], Loss: 0.0936\n",
      "Epoch [5/20], Step [200/252], Loss: 0.1131\n",
      "Training Accuracy: 96.91556674906332 %\n",
      "Validation Accuracy on the 16070 validation images: 93.13048347955946 %\n",
      "\n",
      "Epoch [6/20], Step [100/252], Loss: 0.1229\n",
      "Epoch [6/20], Step [200/252], Loss: 0.0382\n",
      "Training Accuracy: 97.18763117391913 %\n",
      "Validation Accuracy on the 16070 validation images: 94.6611909650924 %\n",
      "\n",
      "Epoch [7/20], Step [100/252], Loss: 0.1069\n",
      "Epoch [7/20], Step [200/252], Loss: 0.0508\n",
      "Training Accuracy: 97.68045644637222 %\n",
      "Validation Accuracy on the 16070 validation images: 95.89944620745442 %\n",
      "Save Checkpoint!!\n",
      "\n",
      "Epoch [8/20], Step [100/252], Loss: 0.0656\n",
      "Epoch [8/20], Step [200/252], Loss: 0.0492\n",
      "Training Accuracy: 97.60894236898154 %\n",
      "Validation Accuracy on the 16070 validation images: 93.26737601891605 %\n",
      "\n",
      "Epoch [9/20], Step [100/252], Loss: 0.0777\n",
      "Epoch [9/20], Step [200/252], Loss: 0.0584\n",
      "Training Accuracy: 98.19504687281376 %\n",
      "Validation Accuracy on the 16070 validation images: 95.99900441789559 %\n",
      "Save Checkpoint!!\n",
      "\n",
      "Epoch [10/20], Step [100/252], Loss: 0.0373\n",
      "Epoch [10/20], Step [200/252], Loss: 0.0435\n",
      "Training Accuracy: 98.26189698863548 %\n",
      "Validation Accuracy on the 16070 validation images: 95.33320888557029 %\n",
      "\n",
      "Epoch [11/20], Step [100/252], Loss: 0.1007\n",
      "Epoch [11/20], Step [200/252], Loss: 0.0450\n",
      "Training Accuracy: 98.56971845218662 %\n",
      "Validation Accuracy on the 16070 validation images: 93.22381930184805 %\n",
      "\n",
      "Epoch [12/20], Step [100/252], Loss: 0.0656\n",
      "Epoch [12/20], Step [200/252], Loss: 0.0357\n",
      "Training Accuracy: 98.70186402997372 %\n",
      "Validation Accuracy on the 16070 validation images: 95.40165515524859 %\n",
      "\n",
      "Epoch [13/20], Step [100/252], Loss: 0.0249\n",
      "Epoch [13/20], Step [200/252], Loss: 0.0164\n",
      "Training Accuracy: 98.7251838378185 %\n",
      "Validation Accuracy on the 16070 validation images: 96.52790741086429 %\n",
      "Save Checkpoint!!\n",
      "\n",
      "Epoch [14/20], Step [100/252], Loss: 0.0398\n",
      "Epoch [14/20], Step [200/252], Loss: 0.0142\n",
      "Training Accuracy: 98.82157237691028 %\n",
      "Validation Accuracy on the 16070 validation images: 96.23545516769336 %\n",
      "\n",
      "Epoch [15/20], Step [100/252], Loss: 0.0355\n",
      "Epoch [15/20], Step [200/252], Loss: 0.0065\n",
      "Training Accuracy: 98.79514326135286 %\n",
      "Validation Accuracy on the 16070 validation images: 96.21056561508307 %\n",
      "\n",
      "Epoch [16/20], Step [100/252], Loss: 0.0674\n",
      "Epoch [16/20], Step [200/252], Loss: 0.0407\n",
      "Training Accuracy: 98.9210702237147 %\n",
      "Validation Accuracy on the 16070 validation images: 96.05500591126875 %\n",
      "\n",
      "Epoch [17/20], Step [100/252], Loss: 0.0167\n",
      "Epoch [17/20], Step [200/252], Loss: 0.0311\n",
      "Training Accuracy: 98.98947499339272 %\n",
      "Validation Accuracy on the 16070 validation images: 96.27901188476137 %\n",
      "\n",
      "Epoch [18/20], Step [100/252], Loss: 0.0369\n",
      "Epoch [18/20], Step [200/252], Loss: 0.0256\n",
      "Training Accuracy: 99.00191222424327 %\n",
      "Validation Accuracy on the 16070 validation images: 96.19189845062535 %\n",
      "\n",
      "Epoch [19/20], Step [100/252], Loss: 0.0280\n",
      "Epoch [19/20], Step [200/252], Loss: 0.0089\n",
      "Training Accuracy: 99.16515087915676 %\n",
      "Validation Accuracy on the 16070 validation images: 96.47190591749113 %\n",
      "\n",
      "Epoch [20/20], Step [100/252], Loss: 0.0414\n",
      "Epoch [20/20], Step [200/252], Loss: 0.0211\n",
      "Training Accuracy: 98.93972606999051 %\n",
      "Validation Accuracy on the 16070 validation images: 95.88700143114927 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Record Training Information\n",
    "total_step = len(imgLoader)\n",
    "loss_list, acc_list, vacc_list = [], [], []\n",
    "best_acc = 0\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Stage\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    for i, (images, labels) in enumerate(imgLoader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device, dtype=torch.long)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            loss_list.append(loss.item())\n",
    "    print('Training Accuracy: {} %'.format(100 * correct / total))\n",
    "    acc_list.append(100 * correct / total)\n",
    "    \n",
    "    # Validation Stage\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in valLoader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device, dtype=torch.long)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Save model checkpoint\n",
    "    vacc = 100*correct/total\n",
    "    print('Validation Accuracy on the 16070 validation images: {} %'.format(vacc))\n",
    "    if vacc > best_acc:\n",
    "        best_acc = vacc\n",
    "        torch.save(model.state_dict(), 'new_convnext_all_calss_epoch_20_weight.ckpt')\n",
    "        print(\"Save Checkpoint!!\")\n",
    "    print()\n",
    "    vacc_list.append(100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e729e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
